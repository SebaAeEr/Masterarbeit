
@article{gopalani_comparing_2015,
	title = {Comparing {Apache} {Spark} and {Map} {Reduce} with {Performance} {Analysis} using {K}-{Means}},
	volume = {113},
	doi = {10.5120/19788-0531},
	abstract = {Data has long been the topic of fascination for Computer Science enthusiasts around the world, and has gained even more prominence in the recent times with the continuous explosion of data resulting from the likes of social media and the quest for tech giants to gain access to deeper analysis of their data. This paper discusses two of the comparison of - Hadoop Map Reduce and the recently introduced Apache Spark - both of which provide a processing model for analyzing big data. Although both of these options are based on the concept of Big Data, their performance varies significantly based on the use case under implementation. This is what makes these two options worthy of analysis with respect to their variability and variety in the dynamic field of Big Data. In this paper we compare these two frameworks along with providing the performance analysis using a standard machine learning algorithm for clustering (K- Means).},
	journal = {International Journal of Computer Applications},
	author = {Gopalani, Satish and Arora, Rohan},
	month = mar,
	year = {2015},
	pages = {8--11},
	file = {Full Text:C\:\\Users\\SebtheLegend\\Zotero\\storage\\Q4XT9C6F\\Gopalani and Arora - 2015 - Comparing Apache Spark and Map Reduce with Perform.pdf:application/pdf},
}

@misc{noauthor_cluster_nodate,
	title = {Cluster {Mode} {Overview} - {Spark} 3.5.1 {Documentation}},
	url = {https://spark.apache.org/docs/latest/cluster-overview.html},
	urldate = {2024-04-30},
	file = {Cluster Mode Overview - Spark 3.5.1 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\SPKC8N36\\cluster-overview.html:text/html},
}

@misc{noauthor_job_nodate,
	title = {Job {Scheduling} - {Spark} 3.5.1 {Documentation}},
	url = {https://spark.apache.org/docs/latest/job-scheduling.html},
	urldate = {2024-04-30},
	file = {Job Scheduling - Spark 3.5.1 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\AL45KLN4\\job-scheduling.html:text/html},
}

@inproceedings{zaharia_resilient_2012,
	address = {USA},
	series = {{NSDI}'12},
	title = {Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing},
	shorttitle = {Resilient distributed datasets},
	abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
	urldate = {2024-04-30},
	booktitle = {Proceedings of the 9th {USENIX} conference on {Networked} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
	month = apr,
	year = {2012},
	pages = {2},
}

@inproceedings{zaharia_spark_2010,
	address = {USA},
	series = {{HotCloud}'10},
	title = {Spark: cluster computing with working sets},
	shorttitle = {Spark},
	abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
	urldate = {2024-04-30},
	booktitle = {Proceedings of the 2nd {USENIX} conference on {Hot} topics in cloud computing},
	publisher = {USENIX Association},
	author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
	month = jun,
	year = {2010},
	pages = {10},
}

@misc{noauthor_monitoring_nodate,
	title = {Monitoring and {Instrumentation} - {Spark} 3.5.1 {Documentation}},
	url = {https://spark.apache.org/docs/latest/monitoring.html},
	urldate = {2024-05-10},
	file = {Monitoring and Instrumentation - Spark 3.5.1 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\JGKK2AHV\\monitoring.html:text/html},
}

@inproceedings{kweun_pokemem_2022,
	title = {{PokéMem}: {Taming} {Wild} {Memory} {Consumers} in {Apache} {Spark}},
	shorttitle = {{PokéMem}},
	url = {https://ieeexplore.ieee.org/abstract/document/9820670},
	doi = {10.1109/IPDPS53621.2022.00015},
	abstract = {Apache Spark is a widely used in-memory processing system due to its high performance. For fast data processing, Spark manages in-memory data such as cached or shuffling (aggregate and sorting) data in its own managed memory pools. However, despite its sophisticated memory management scheme, we found that Spark still suffers from out-of-memory (OOM) exceptions and high garbage collection (GC) overheads when wild memory consumers, who are not tracked by Spark and execute external codes, use a large amount of memory. To resolve the problems, we propose PokéMem, which is an enhanced Spark that incorporates wild memory consumers into the managed ones to prevent them from taking up memory spaces excessively in stealth. Our main idea is to open the black-box of unmanaged memory regions in external codes by providing customized data collections. PokéMem enables fine-grained controls of created objects within running tasks, by spilling and reloading the objects of custom data collections based on the memory pressure and access patterns. To further reduce memory pressures, PokéMem exploits pre-built memory estimation models to predict the external code's memory usage and proactively acquires memory before the execution of external code, and also performs JVM heap-usage monitoring to avoid critical memory pressures. With the help of these techniques, our evaluations show that PokéMem outperforms vanilla Spark with at most 3× faster execution with 3.9× smaller GC overheads, and successfully runs workloads without OOM exception that vanilla Spark has failed to run.},
	urldate = {2024-05-10},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Kweun, Minhyeok and Kim, Goeun and Oh, Byungsoo and Jung, Seongho and Um, Taegeon and Lee, Woo-Yeon},
	month = may,
	year = {2022},
	note = {ISSN: 1530-2075},
	keywords = {Apache Spark, Cluster computing, Codes, distributed framework, Machine learning algorithms, memory management, Memory management, Predictive models, Runtime, Sparks},
	pages = {59--69},
}

@misc{noauthor_interruptible_nodate,
	title = {Interruptible tasks {\textbar} {Proceedings} of the 25th {Symposium} on {Operating} {Systems} {Principles}},
	url = {https://dl.acm.org/doi/10.1145/2815400.2815407},
	urldate = {2024-05-10},
	file = {Interruptible tasks | Proceedings of the 25th Symposium on Operating Systems Principles:C\:\\Users\\SebtheLegend\\Zotero\\storage\\SCQX5KGS\\2815400.html:text/html},
}

@inproceedings{xu_experience_2015,
	title = {Experience report: {A} characteristic study on out of memory errors in distributed data-parallel applications},
	shorttitle = {Experience report},
	url = {https://ieeexplore.ieee.org/document/7381844/keywords#keywords},
	doi = {10.1109/ISSRE.2015.7381844},
	abstract = {Out of memory (OOM) errors occur frequently in data-intensive applications that run atop distributed data-parallel frameworks, such as MapReduce and Spark. In these applications, the memory space is shared by the framework and user code. Since the framework hides the details of distributed execution, it is challenging for users to pinpoint the root causes and fix these OOM errors. This paper presents a comprehensive characteristic study on 123 real-world OOM errors in Hadoop and Spark applications. Our major findings include: (1) 12\% errors are caused by the large data buffered/cached in the framework, which indicates that it is hard for users to configure the right memory quota to balance the memory usage of the framework and user code. (2) 37\% errors are caused by the unexpected large runtime data, such as large data partition, hotspot key, and large key/value record. (3) Most errors (64\%) are caused by memory-consuming user code, which carelessly processes unexpected large data or generates large in-memory computing results. Among them, 13\% errors are also caused by the unexpected large runtime data. (4) There are three common fix patterns (used in 34\% errors), namely changing the memory/dataflow-related configurations, dividing runtime data, and optimizing user code logic. Our findings inspire us to propose potential solutions to avoid the OOM errors: (1) providing dynamic memory management mechanisms to balance the memory usage of the framework and user code at runtime; (2) providing users with memory+disk data structures, since accumulating large computing results in in-memory data structures is a common cause (15\% errors).},
	urldate = {2024-05-10},
	booktitle = {2015 {IEEE} 26th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Xu, Lijie and Dou, Wensheng and Zhu, Feng and Gao, Chushu and Liu, Jie and Zhong, Hua and Wei, Jun},
	month = nov,
	year = {2015},
	keywords = {Memory management, Runtime, Sparks, characteristic study, Data structures, Fault tolerance, Fault tolerant systems, MapReduce, out of memory, Programming},
	pages = {518--529},
}

@inproceedings{fang_interruptible_2015,
	address = {New York, NY, USA},
	series = {{SOSP} '15},
	title = {Interruptible tasks: treating memory pressure as interrupts for highly scalable data-parallel programs},
	isbn = {978-1-4503-3834-9},
	shorttitle = {Interruptible tasks},
	url = {https://dl.acm.org/doi/10.1145/2815400.2815407},
	doi = {10.1145/2815400.2815407},
	abstract = {Real-world data-parallel programs commonly suffer from great memory pressure, especially when they are executed to process large datasets. Memory problems lead to excessive GC effort and out-of-memory errors, significantly hurting system performance and scalability. This paper proposes a systematic approach that can help data-parallel tasks survive memory pressure, improving their performance and scalability without needing any manual effort to tune system parameters. Our approach advocates interruptible task (ITask), a new type of data-parallel tasks that can be interrupted upon memory pressure---with part or all of their used memory reclaimed---and resumed when the pressure goes away. To support ITasks, we propose a novel programming model and a runtime system, and have instantiated them on two state-of-the-art platforms Hadoop and Hyracks. A thorough evaluation demonstrates the effectiveness of ITask: it has helped real-world Hadoop programs survive 13 out-of-memory problems reported on StackOverflow; a second set of experiments with 5 already well-tuned programs in Hyracks on datasets of different sizes shows that the ITask-based versions are 1.5--3x faster and scale to 3--24x larger datasets than their regular counterparts.},
	urldate = {2024-05-10},
	booktitle = {Proceedings of the 25th {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {Association for Computing Machinery},
	author = {Fang, Lu and Nguyen, Khanh and Xu, Guoqing and Demsky, Brian and Lu, Shan},
	year = {2015},
	pages = {394--409},
	file = {Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\HJF9DFAK\\Fang et al. - 2015 - Interruptible tasks treating memory pressure as i.pdf:application/pdf},
}

@inproceedings{xu_memtune_2016,
	title = {{MEMTUNE}: {Dynamic} {Memory} {Management} for {In}-{Memory} {Data} {Analytic} {Platforms}},
	shorttitle = {{MEMTUNE}},
	url = {https://ieeexplore.ieee.org/abstract/document/7516034},
	doi = {10.1109/IPDPS.2016.105},
	abstract = {Memory is a crucial resource for big data processing frameworks such as Spark and M3R, where the memory is used both for computation and for caching intermediate storage data. Consequently, optimizing memory is the key to extracting high performance. The extant approach is to statically split the memory for computation and caching based on workload profiling. This approach is unable to capture the varying workload characteristics and dynamic memory demands. Another factor that affects caching efficiency is the choice of data placement and eviction policy. The extant LRU policy is oblivious of task scheduling information from the analytic frameworks, and thus can lead to lost optimization opportunities. In this paper, we address the above issues by designing MEMTUNE, a dynamic memory manager for in-memory data analytics. MEMTUNE dynamically tunes computation/caching memory partitions at runtime based on workload memory demand and in-memory data cache needs. Moreover, if needed, the scheduling information from the analytic framework is leveraged to evict data that will not be needed in the near future. Finally, MEMTUNE also supports task-level data prefetching with a configurable window size to more effectively overlap computation with I/O. Our experiments show that MEMTUNE improves memory utilization, yields an overall performance gain of up to 46\%, and achieves cache hit ratio of up to 41\% compared to standard Spark.},
	urldate = {2024-05-17},
	booktitle = {2016 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Xu, Luna and Li, Min and Zhang, Li and Butt, Ali R. and Wang, Yandong and Hu, Zane Zhenhua},
	month = may,
	year = {2016},
	note = {ISSN: 1530-2075},
	keywords = {Memory management, Sparks, Big data, Distributed databases, Distributed processing, Logistics, Tuning},
	pages = {383--392},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\SebtheLegend\\Zotero\\storage\\9M2FYWSV\\7516034.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\462SEPNH\\Xu et al. - 2016 - MEMTUNE Dynamic Memory Management for In-Memory D.pdf:application/pdf},
}

@article{tang_dynamic_2020,
	title = {Dynamic memory-aware scheduling in spark computing environment},
	volume = {141},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S074373151930824X},
	doi = {10.1016/j.jpdc.2020.03.010},
	abstract = {Scheduling plays an important role in improving the performance of big data-parallel processing. Spark is an in-memory parallel computing framework that uses a multi-threaded model in task scheduling. Most Spark task scheduling processes do not take the memory into account, but the number of concurrent task threads determined by the user. It emerges as a potential limitation for the performance. To overcome the limitations in the Spark-core source code, this paper proposes a dynamic Spark memory-aware task scheduler (DMATS), which not only treats memory and network I/O as a computational resource but also dynamically adjusts concurrency when scheduling tasks. Specifically, we first analyze the RDD based Spark execution engine to obtain the amount of task processing data and propose an algorithm for estimating the initial adaptive task concurrency, which is integrated with the known task input information and the executor memory. Then, a dynamic adjustment algorithm is proposed to change the concurrency dynamically through feedback information to optimally utilize the limited memory resources. We implement a dynamic memory-aware task scheduling (DMATS) in Spark 2.3.4 and evaluate performance with two typical benchmarks, shuffle-light and shuffle-heavy. The results show that the algorithm not only reduces the execution time by 43.64\%, but also significantly improves resource utilization. Experiments also show that our proposed method has advantages compared with other similar works such as WASP.},
	urldate = {2024-05-17},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Tang, Zhuo and Zeng, Ailing and Zhang, Xuedong and Yang, Li and Li, Kenli},
	month = jul,
	year = {2020},
	keywords = {Concurrency, Dynamic adjustment, Memory resource, Spark, Task scheduling},
	pages = {10--22},
	file = {ScienceDirect Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\TIFZBJ59\\S074373151930824X.html:text/html},
}

@article{matteussi_performance_2022,
	title = {Performance {Evaluation} {Analysis} of {Spark} {Streaming} {Backpressure} for {Data}-{Intensive} {Pipelines}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/13/4756},
	doi = {10.3390/s22134756},
	abstract = {A significant rise in the adoption of streaming applications has changed the decision-making processes in the last decade. This movement has led to the emergence of several Big Data technologies for in-memory processing, such as the systems Apache Storm, Spark, Heron, Samza, Flink, and others. Spark Streaming, a widespread open-source implementation, processes data-intensive applications that often require large amounts of memory. However, Spark Unified Memory Manager cannot properly manage sudden or intensive data surges and their related in-memory caching needs, resulting in performance and throughput degradation, high latency, a large number of garbage collection operations, out-of-memory issues, and data loss. This work presents a comprehensive performance evaluation of Spark Streaming backpressure to investigate the hypothesis that it could support data-intensive pipelines under specific pressure requirements. The results reveal that backpressure is suitable only for small and medium pipelines for stateless and stateful applications. Furthermore, it points out the Spark Streaming limitations that lead to in-memory-based issues for data-intensive pipelines and stateful applications. In addition, the work indicates potential solutions.},
	language = {en},
	number = {13},
	urldate = {2024-05-17},
	journal = {Sensors},
	author = {Matteussi, Kassiano J. and dos Anjos, Julio C. S. and Leithardt, Valderi R. Q. and Geyer, Claudio F. R.},
	month = jan,
	year = {2022},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {backpressure, big data, spark streaming, stream processing},
	pages = {4756},
	file = {Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\DL6MKLAX\\Matteussi et al. - 2022 - Performance Evaluation Analysis of Spark Streaming.pdf:application/pdf},
}

@misc{noauthor_introducing_2023,
	title = {Introducing {Velox}: {An} open source unified execution engine},
	shorttitle = {Introducing {Velox}},
	url = {https://engineering.fb.com/2023/03/09/open-source/velox-open-source-execution-engine/},
	abstract = {Meta is introducing Velox, an open source unified execution engine aimed at accelerating data management systems and streamlining their development. Velox is under active development. Experimental …},
	language = {en-US},
	urldate = {2024-06-12},
	journal = {Engineering at Meta},
	month = mar,
	year = {2023},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\8JSSRUXE\\velox-open-source-execution-engine.html:text/html},
}

@misc{noauthor_facebookincubatorvelox_2024,
	title = {facebookincubator/velox},
	copyright = {Apache-2.0},
	url = {https://github.com/facebookincubator/velox},
	abstract = {A C++ vectorized database acceleration library aimed to optimizing query engines and data processing systems.},
	urldate = {2024-06-12},
	publisher = {Meta Incubator},
	month = jun,
	year = {2024},
	note = {original-date: 2021-07-23T23:00:23Z},
}

@misc{noauthor_memory_nodate,
	title = {Memory {Management} — {Velox} documentation},
	url = {https://facebookincubator.github.io/velox/develop/memory.html},
	urldate = {2024-06-12},
	file = {Memory Management — Velox documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\JGYEHQBW\\memory.html:text/html},
}

@misc{noauthor_velox_nodate,
	title = {Velox: {Meta}’s {Unified} {Execution} {Engine} - {Meta} {Research}},
	shorttitle = {Velox},
	url = {https://research.facebook.com/publications/velox-metas-unified-execution-engine/},
	abstract = {Velox provides reusable, extensible, high-performance, and dialect-agnostic data processing components for building execution engines, and enhancing data management systems.},
	language = {de},
	urldate = {2024-06-12},
	journal = {Meta Research},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\4X62HTGC\\velox-metas-unified-execution-engine.html:text/html},
}

@misc{noauthor_spilling_nodate,
	title = {Spilling — {Velox} documentation},
	url = {https://facebookincubator.github.io/velox/develop/spilling.html},
	urldate = {2024-06-12},
	file = {Spilling — Velox documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\VMBDUDBS\\spilling.html:text/html},
}

@misc{noauthor_tuning_nodate,
	title = {Tuning - {Spark} 3.5.1 {Documentation}},
	url = {https://spark.apache.org/docs/latest/tuning.html},
	urldate = {2024-06-12},
	file = {Tuning - Spark 3.5.1 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\BDE9XQMK\\tuning.html:text/html},
}

@misc{noauthor_citusdatacitus_2024,
	title = {citusdata/citus},
	copyright = {AGPL-3.0},
	url = {https://github.com/citusdata/citus},
	abstract = {Distributed PostgreSQL as an extension},
	urldate = {2024-06-13},
	publisher = {Citus Data},
	month = jun,
	year = {2024},
	note = {original-date: 2016-02-01T21:58:39Z},
	keywords = {citus, citus-extension, database, database-cluster, distributed-database, multi-tenant, postgres, postgresql, relational-database, scale, sharding, sql},
}

@misc{noauthor_concepts_nodate,
	title = {Concepts - {Citus} 12.1 documentation},
	url = {https://docs.citusdata.com/en/v12.1/get_started/concepts.html},
	abstract = {Docs for the Citus open source extension to Postgres. Citus gives you Postgres at any scale—from a single node to a large distributed database cluster. Available as 100\% open source \& as a managed service with Azure Cosmos DB for PostgreSQL.},
	language = {en},
	urldate = {2024-06-13},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\LPWHTC68\\concepts.html:text/html},
}

@misc{noauthor_query_nodate,
	title = {Query {Performance} {Tuning} - {Citus} 12.1 documentation},
	url = {https://docs.citusdata.com/en/v12.1/performance/performance_tuning.html},
	abstract = {Docs for the Citus open source extension to Postgres. Citus gives you Postgres at any scale—from a single node to a large distributed database cluster. Available as 100\% open source \& as a managed service with Azure Cosmos DB for PostgreSQL.},
	language = {en},
	urldate = {2024-06-13},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\5N8EXJAY\\performance_tuning.html:text/html},
}

@misc{noauthor_194_2024,
	title = {19.4. {Managing} {Kernel} {Resources}},
	url = {https://www.postgresql.org/docs/16/kernel-resources.html},
	abstract = {19.4.\&nbsp;Managing Kernel Resources \# 19.4.1. Shared Memory and Semaphores 19.4.2. systemd RemoveIPC 19.4.3. Resource Limits 19.4.4. Linux Memory Overcommit 19.4.5. …},
	language = {en},
	urldate = {2024-06-13},
	journal = {PostgreSQL Documentation},
	month = may,
	year = {2024},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\YH8ST3LJ\\kernel-resources.html:text/html},
}

@misc{noauthor_204_2024,
	title = {20.4. {Resource} {Consumption}},
	url = {https://www.postgresql.org/docs/16/runtime-config-resource.html},
	abstract = {20.4.\&nbsp;Resource Consumption \# 20.4.1. Memory 20.4.2. Disk 20.4.3. Kernel Resource Usage 20.4.4. Cost-based Vacuum Delay 20.4.5. Background Writer 20.4.6. Asynchronous …},
	language = {en},
	urldate = {2024-06-13},
	journal = {PostgreSQL Documentation},
	month = may,
	year = {2024},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\JFHVU54T\\runtime-config-resource.html:text/html},
}

@misc{ltd_postgresql_2024,
	title = {{PostgreSQL} {Spill} to {Disk} {Recommendations} in {Redgate} {Monitor}},
	url = {https://www.red-gate.com/hub/product-learning/redgate-monitor/postgresql-spill-to-disk-recommendations-in-redgate-monitor},
	abstract = {When processing a query in PostgreSQL that requires more memory than it's configured to allocate, data will be spilled to disk to complete the query operations. This can introduce performance bottlenecks. Redgate Monitor now includes recommendations for PostgreSQL that warn you when a spill occurs and will help you understand why and what you can do to avoid it.},
	urldate = {2024-06-13},
	journal = {Redgate},
	author = {Ltd, Red Gate Software and Lopez, Francisco},
	month = apr,
	year = {2024},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\8HRTHEN9\\postgresql-spill-to-disk-recommendations-in-redgate-monitor.html:text/html},
}

@misc{noauthor_use_nodate,
	title = {Use cases — {Trino} 449 {Documentation}},
	url = {https://trino.io/docs/current/overview/use-cases.html},
	urldate = {2024-06-13},
	file = {Use cases — Trino 449 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\CU3PHJXR\\use-cases.html:text/html},
}

@misc{noauthor_trino_nodate,
	title = {Trino concepts — {Trino} 449 {Documentation}},
	url = {https://trino.io/docs/current/overview/concepts.html},
	urldate = {2024-06-13},
	file = {Trino concepts — Trino 449 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\8BNFN6MB\\concepts.html:text/html},
}

@misc{noauthor_cost_nodate,
	title = {Cost in {EXPLAIN} — {Trino} 449 {Documentation}},
	url = {https://trino.io/docs/current/optimizer/cost-in-explain.html},
	urldate = {2024-06-13},
	file = {Cost in EXPLAIN — Trino 449 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\HHBGV9YC\\cost-in-explain.html:text/html},
}

@misc{noauthor_spill_nodate,
	title = {Spill to disk — {Trino} 449 {Documentation}},
	url = {https://trino.io/docs/current/admin/spill.html},
	urldate = {2024-06-13},
	file = {Spill to disk — Trino 449 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\LMPKBH8M\\spill.html:text/html},
}

@misc{noauthor_overview_nodate,
	title = {Overview of {ClickHouse} {Architecture} {\textbar} {ClickHouse} {Docs}},
	url = {https://clickhouse.com/docs/en/development/architecture},
	abstract = {ClickHouse is a true column-oriented DBMS. Data is stored by columns, and during the execution of arrays (vectors or chunks of columns).},
	language = {en},
	urldate = {2024-06-27},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\E5ZVNPYP\\architecture.html:text/html},
}

@misc{clickhouse_fast_nodate,
	title = {Fast {Open}-{Source} {OLAP} {DBMS}},
	url = {https://clickhouse.com},
	abstract = {ClickHouse is a fast open-source column-oriented database management system that allows generating analytical data reports in real-time using SQL queries},
	language = {en},
	urldate = {2024-06-27},
	journal = {ClickHouse},
	author = {ClickHouse},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\HS6VBULD\\clickhouse.com.html:text/html},
}

@misc{noauthor_foundationdb_nodate,
	title = {{FoundationDB} 7.1 — {FoundationDB} 7.1},
	url = {https://apple.github.io/foundationdb/},
	urldate = {2024-06-28},
	file = {FoundationDB 7.1 — FoundationDB 7.1:C\:\\Users\\SebtheLegend\\Zotero\\storage\\VGQGSYMI\\foundationdb.html:text/html},
}

@misc{noauthor_architecture_nodate,
	title = {Architecture — {FoundationDB} 7.1},
	url = {https://apple.github.io/foundationdb/architecture.html},
	urldate = {2024-06-28},
	file = {Architecture — FoundationDB 7.1:C\:\\Users\\SebtheLegend\\Zotero\\storage\\N779PP68\\architecture.html:text/html},
}

@misc{noauthor_dynamic_nodate,
	title = {Dynamic filtering — {Trino} 453 {Documentation}},
	url = {https://trino.io/docs/current/admin/dynamic-filtering.html},
	urldate = {2024-07-30},
	file = {Dynamic filtering — Trino 453 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\JSHYRVPW\\dynamic-filtering.html:text/html},
}

@misc{lanzelot_how_2021,
	type = {Forum post},
	title = {How to determine {CPU} and memory consumption from inside a process},
	url = {https://stackoverflow.com/q/63166},
	urldate = {2024-10-16},
	journal = {Stack Overflow},
	author = {Lanzelot},
	month = jul,
	year = {2021},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\ALEQL2WB\\how-to-determine-cpu-and-memory-consumption-from-inside-a-process.html:text/html},
}

@misc{lanzelot_answer_2008,
	title = {Answer to "{How} to determine {CPU} and memory consumption from inside a process"},
	url = {https://stackoverflow.com/a/64166},
	urldate = {2024-10-16},
	journal = {Stack Overflow},
	author = {Lanzelot},
	month = sep,
	year = {2008},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\K99D35XR\\how-to-determine-cpu-and-memory-consumption-from-inside-a-process.html:text/html},
}

@inproceedings{vuppalapati_building_2020,
	title = {Building {An} {Elastic} {Query} {Engine} on {Disaggregated} {Storage}},
	isbn = {978-1-939133-13-7},
	url = {https://www.usenix.org/conference/nsdi20/presentation/vuppalapati},
	language = {en},
	urldate = {2025-02-09},
	author = {Vuppalapati, Midhul and Miron, Justin and Agarwal, Rachit and Truong, Dan and Motivala, Ashish and Cruanes, Thierry},
	year = {2020},
	pages = {449--462},
	file = {Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\A9H3UYQY\\Vuppalapati et al. - 2020 - Building An Elastic Query Engine on Disaggregated .pdf:application/pdf},
}

@inproceedings{gantz_digital_2012,
	title = {The {Digital} {Universe} in 2020: {Big} {Data}, {Bigger} {Digital} {Shadows}, and {Biggest} {Growth} in the {Far} {East}},
	shorttitle = {The {Digital} {Universe} in 2020},
	url = {https://www.semanticscholar.org/paper/The-Digital-Universe-in-2020%3A-Big-Data%2C-Bigger-and-Gantz-Reinsel/cd6fc4c68e4e6f3ddee2a2e07f6341a31926e403},
	abstract = {Semantic Scholar extracted view of "The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East" by J. Gantz et al.},
	urldate = {2025-02-09},
	author = {Gantz, J. and Reinsel, David},
	year = {2012},
}

@article{philip_chen_data-intensive_2014,
	title = {Data-intensive applications, challenges, techniques and technologies: {A} survey on {Big} {Data}},
	volume = {275},
	issn = {0020-0255},
	shorttitle = {Data-intensive applications, challenges, techniques and technologies},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025514000346},
	doi = {10.1016/j.ins.2014.01.015},
	abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.},
	urldate = {2025-02-09},
	journal = {Information Sciences},
	author = {Philip Chen, C. L. and Zhang, Chun-Yang},
	month = aug,
	year = {2014},
	keywords = {Big Data, Cloud computing, Data-intensive computing, e-Science, Parallel and distributed computing},
	pages = {314--347},
	file = {ScienceDirect Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\Q24TJWBT\\S0020025514000346.html:text/html},
}

@article{kossmann_state_2000,
	title = {The state of the art in distributed query processing},
	volume = {32},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/371578.371598},
	doi = {10.1145/371578.371598},
	abstract = {Distributed data processing is becoming a reality. Businesses want to do it for many reasons, and they often must do it in order to stay competitive. While much of the infrastructure for distributed data processing is already there (e.g., modern network technology), a number of issues make distributed data processing still a complex undertaking: (1) distributed systems can become very large, involving thousands of heterogeneous sites including PCs and mainframe server machines; (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system; (3) legacy systems need to be integrated—such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. This paper presents the state of the art of query processing for distributed database and information systems.  The paper presents the “textbook” architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intraquery paralleli sm, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses different kinds of distributed systems such as client-server, middleware (multitier), and heterogeneous database systems, and shows how query processing works in these systems.},
	number = {4},
	urldate = {2025-02-09},
	journal = {ACM Comput. Surv.},
	author = {Kossmann, Donald},
	year = {2000},
	pages = {422--469},
	file = {Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\LBAMTBPQ\\Kossmann - 2000 - The state of the art in distributed query processi.pdf:application/pdf},
}

@article{kersten_everything_2018,
	title = {Everything you always wanted to know about compiled and vectorized queries but were afraid to ask},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3275366.3284966},
	doi = {10.14778/3275366.3284966},
	abstract = {The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-specific choices obstruct a direct comparison of architectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We find that both are efficient, but have different strengths and weaknesses. Vectorization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which benefits cache-resident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.},
	number = {13},
	urldate = {2025-02-09},
	journal = {Proc. VLDB Endow.},
	author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
	month = sep,
	year = {2018},
	pages = {2209--2222},
	file = {Submitted Version:C\:\\Users\\SebtheLegend\\Zotero\\storage\\C6JFHKPV\\Kersten et al. - 2018 - Everything you always wanted to know about compile.pdf:application/pdf},
}

@misc{sundarmurthy_providing_2020,
	title = {Providing {Insights} for {Queries} affected by {Failures} and {Stragglers}},
	url = {http://arxiv.org/abs/2002.01531},
	doi = {10.48550/arXiv.2002.01531},
	abstract = {Interactive time responses are a crucial requirement for users analyzing large amounts of data. Such analytical queries are typically run in a distributed setting, with data being sharded across thousands of nodes for high throughput. However, providing real-time analytics is still a very big challenge; with data distributed across thousands of nodes, the probability that some of the required nodes are unavailable or very slow during query execution is very high and unavailability may result in slow execution or even failures. The sheer magnitude of data and users increase resource contention and this exacerbates the phenomenon of stragglers and node failures during execution. In this paper, we propose a novel solution to alleviate the straggler/failure problem that exploits existing efficient partitioning properties of the data, particularly, co-hash partitioned data, and provides approximate answers along with confidence bounds to queries affected by failed/straggler nodes. We consider aggregate queries that involve joins, group bys, having clauses and a subclass of nested subqueries. Finally, we validate our approach through extensive experiments on the TPC-H dataset.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Sundarmurthy, Bruhathi and Deshmukh, Harshad and Koutris, Paris and Naughton, Jeffrey},
	month = feb,
	year = {2020},
	note = {arXiv:2002.01531 [cs]},
	keywords = {Computer Science - Databases},
	file = {Preprint PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\ZVV8IUBT\\Sundarmurthy et al. - 2020 - Providing Insights for Queries affected by Failure.pdf:application/pdf;Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\MC68Z94D\\2002.html:text/html},
}

@misc{noauthor_trino_nodate-1,
	title = {Trino documentation — {Trino} 470 {Documentation}},
	url = {https://trino.io/docs/current/},
	urldate = {2025-02-09},
	file = {Trino documentation — Trino 470 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\4IXVXUJV\\current.html:text/html},
}

@misc{noauthor_overview_nodate-1,
	title = {Overview - {Spark} 3.5.4 {Documentation}},
	url = {https://spark.apache.org/docs/latest/index.html},
	urldate = {2025-02-09},
	file = {Overview - Spark 3.5.4 Documentation:C\:\\Users\\SebtheLegend\\Zotero\\storage\\WQCPDCDD\\index.html:text/html},
}

@article{melnik_dremel_2010,
	title = {Dremel: interactive analysis of web-scale datasets},
	volume = {3},
	issn = {2150-8097},
	shorttitle = {Dremel},
	url = {https://doi.org/10.14778/1920841.1920886},
	doi = {10.14778/1920841.1920886},
	abstract = {Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.},
	number = {1-2},
	urldate = {2025-02-09},
	journal = {Proc. VLDB Endow.},
	author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo},
	month = sep,
	year = {2010},
	pages = {330--339},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {The} {SAP} {HANA} database - {An} architecture overview},
	url = {https://www.researchgate.net/publication/224994304_The_SAP_HANA_database_-_An_architecture_overview},
	abstract = {PDF {\textbar} Requirements of enterprise applications have become much more demanding because they execute complex reports on transactional data while thousands... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-02-09},
	journal = {ResearchGate},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\UQEXKIV4\\224994304_The_SAP_HANA_database_-_An_architecture_overview.html:text/html},
}

@inproceedings{perron_starling_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Starling: {A} {Scalable} {Query} {Engine} on {Cloud} {Functions}},
	isbn = {978-1-4503-6735-6},
	shorttitle = {Starling},
	url = {https://dl.acm.org/doi/10.1145/3318464.3380609},
	doi = {10.1145/3318464.3380609},
	abstract = {Much like on-premises systems, the natural choice for running database analytics workloads in the cloud is to provision a cluster of nodes to run a database instance. However, analytics workloads are often bursty or low volume, leaving clusters idle much of the time, meaning customers pay for compute resources even when underutilized. The ability of cloud function services, such as AWS Lambda or Azure Functions, to run small, fine granularity tasks make them appear to be a natural choice for query processing in such settings. But implementing an analytics system on cloud functions comes with its own set of challenges. These include managing hundreds of tiny stateless resource-constrained workers, handling stragglers, and shuffling data through opaque cloud services. In this paper we present Starling, a query execution engine built on cloud function services that employs a number of techniques to mitigate these challenges, providing interactive query latency at a lower total cost than provisioned systems with low-to-moderate utilization. In particular, on a 1TB TPC-H dataset in cloud storage, Starling is less expensive than the best provisioned systems for workloads when queries arrive 1 minute apart or more. Starling also has lower latency than competing systems reading from cloud object stores and can scale to larger datasets.},
	urldate = {2025-02-09},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Perron, Matthew and Castro Fernandez, Raul and DeWitt, David and Madden, Samuel},
	year = {2020},
	pages = {131--141},
	file = {Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\SJTEVLDY\\Perron et al. - 2020 - Starling A Scalable Query Engine on Cloud Functio.pdf:application/pdf},
}

@misc{user_documentation_nodate,
	title = {Documentation},
	url = {https://duckdb.org/docs/},
	abstract = {Connecting to DuckDB DuckDB connection overview Client APIs CLI (command line interface) Java Python R WebAssembly All client APIs SQL Introduction Statements Other Guides Installation Building DuckDB Browsing offline},
	language = {en},
	urldate = {2025-02-09},
	journal = {DuckDB},
	author = {User, GitHub},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\BQ65DFM8\\docs.html:text/html},
}

@article{kuschewski_high-performance_2024,
	title = {High-{Performance} {Query} {Processing} with {NVMe} {Arrays}: {Spilling} without {Killing} {Performance}},
	volume = {2},
	shorttitle = {High-{Performance} {Query} {Processing} with {NVMe} {Arrays}},
	url = {https://dl.acm.org/doi/10.1145/3698813},
	doi = {10.1145/3698813},
	abstract = {This paper aims to bridge the gap between fast in-memory query engines and slow but robust engines that can utilize external storage. We find that current systems have to choose between fast in-memory operators and slower out-of-memory operators. We present a solution that leverages two independent but complementary techniques: First, we propose adaptive materialization, which can turn any hash-based in-memory operator into an out-of-memory operator without reducing in-memory performance. Second, we introduce self-regulating compression, which optimizes the throughput of spilling operators based on the current workload and available hardware. We evaluate these techniques using the prototype query engine Spilly, which matches the performance of state-of-the-art in-memory systems, but also efficiently executes large out-of-memory workloads by spilling to NVMe arrays.},
	number = {6},
	urldate = {2025-02-13},
	journal = {Proc. ACM Manag. Data},
	author = {Kuschewski, Maximilian and Giceva, Jana and Neumann, Thomas and Leis, Viktor},
	year = {2024},
	pages = {238:1--238:27},
	file = {Full Text PDF:C\:\\Users\\SebtheLegend\\Zotero\\storage\\CASUDLUM\\Kuschewski et al. - 2024 - High-Performance Query Processing with NVMe Arrays.pdf:application/pdf},
}

@misc{noauthor_umbra_nodate,
	title = {The {Umbra} {Database} {System}},
	url = {https://umbra-db.com/},
	abstract = {Umbra is a high performance database system for flash-based storage with in-memory speed, developed at the Technische Universität München.},
	language = {en-us},
	urldate = {2025-02-13},
	file = {Snapshot:C\:\\Users\\SebtheLegend\\Zotero\\storage\\BWPT7SFG\\umbra-db.com.html:text/html},
}
